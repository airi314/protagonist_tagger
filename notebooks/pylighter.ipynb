{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "promising-induction",
   "metadata": {},
   "source": [
    "## Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "foster-hypothetical",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('..')\n",
    "\n",
    "import json\n",
    "import pandas as pd\n",
    "from pylighter import Annotation\n",
    "import ast\n",
    "import tabulate\n",
    "\n",
    "from tool.gender_checker import get_personal_titles\n",
    "from tool.pylighter_utils import annotations_to_pylighter, csv_to_json\n",
    "from tool.annotations_utils import read_annotations, has_intersection, fix_personal_titles, personal_titles_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "preceding-eleven",
   "metadata": {},
   "source": [
    "## Fixing gold_standard annotations by cutting personal_titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "emerging-parts",
   "metadata": {},
   "outputs": [],
   "source": [
    "gold_standard_path = \"data/testing_sets/test_person_gold_standard\"\n",
    "new_gold_standard_path = \"data/testing_sets/test_person_gold_standard_titles\"\n",
    "os.makedirs(new_gold_standard_path)\n",
    "results_annotated = {}\n",
    "results_not_annotated = {}\n",
    "for title in os.listdir(gold_standard_path):\n",
    "    annotations = read_annotations(os.path.join(gold_standard_path, title))\n",
    "    annotations = fix_personal_titles(annotations)\n",
    "    with open(os.path.join(new_gold_standard_path, title), 'w') as f:\n",
    "        f.write(json.dumps(annotations))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hired-corps",
   "metadata": {},
   "source": [
    "## Calculating errors statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cooked-sociology",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_errors_stats(path_1, path_2, path_3):\n",
    "    results = []\n",
    "    for title in os.listdir(os.path.join(path_1))[:1]:\n",
    "        annotations = read_annotations(os.path.join(path_1, title))\n",
    "        personal_title_annotated, _ = personal_titles_stats(annotations)\n",
    "        titles_annotated_count = sum(list(personal_title_annotated.values()))\n",
    "        annotations_2 = read_annotations(os.path.join(path_2, title))\n",
    "        annotations_3 = read_annotations(os.path.join(path_3, title))\n",
    "\n",
    "        old_count = 0\n",
    "        new_count = 0\n",
    "        exact_count = 0\n",
    "        intersections_count = 0\n",
    "        incorrect_count = 0\n",
    "        missing_count = 0\n",
    "\n",
    "        for anno, anno2 in zip(annotations_2, annotations_3):\n",
    "            entities = anno['entities']\n",
    "            entities2 = anno2['entities']\n",
    "\n",
    "\n",
    "            old_count += len(entities)\n",
    "            new_count += len(entities2)\n",
    "\n",
    "            matched_count = 0\n",
    "            for ent1 in entities:\n",
    "                exact = False\n",
    "                intersection = False\n",
    "\n",
    "                for ent2 in entities2:\n",
    "                    if ent1 == ent2:\n",
    "                        exact_count += 1\n",
    "                        exact = True\n",
    "                        matched_count += 1\n",
    "\n",
    "                if not exact:\n",
    "                    for ent2 in entities2:\n",
    "                        if has_intersection(ent1, ent2):\n",
    "                            intersections_count += 1\n",
    "                            intersection = True\n",
    "                            matched_count += 1\n",
    "\n",
    "                if not exact and not intersection:\n",
    "                    incorrect_count += 1\n",
    "\n",
    "            missing_count += (len(entities2) - matched_count)\n",
    "\n",
    "        exact_count -= titles_annotated_count\n",
    "\n",
    "        title_results = {'Title': title.split('.')[0].replace('_', ' '), \n",
    "                         '# previously annotated': old_count, \n",
    "                         '# correct annotations': exact_count, \n",
    "                         '# personal titles annotated': titles_annotated_count, \n",
    "                         '# annotations with wrong boundaries (except personal titles)': intersections_count, \n",
    "                         '# missing annotations':  missing_count}\n",
    "        results.append(title_results)\n",
    "    return pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "quantitative-gallery",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = get_errors_stats(\"data/testing_sets/test_person_gold_standard\",\n",
    "                \"data/testing_sets/test_person_gold_standard_titles_corrected\",\n",
    "                \"data/testing_sets/test_person_gold_standard_corrected\")\n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "composite-garage",
   "metadata": {},
   "outputs": [],
   "source": [
    "table_latex = results_df.to_latex(index=False)\n",
    "table_latex = table_latex.split('\\n')\n",
    "table_latex = [' & '.join([x.strip() for x in line.split('&')]) for line in table_latex]\n",
    "table_latex[2] = ' & '.join(['\\\\rot{\\\\textbf{' + x + '}}' for x in table_latex[2].split('&')])\n",
    "print('\\n'.join(table_latex))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "composed-problem",
   "metadata": {},
   "source": [
    "## Correcting annonations with pylighter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "thousand-criticism",
   "metadata": {},
   "outputs": [],
   "source": [
    "annotations = read_annotations(os.path.join(\"data/testing_sets/test_person_gold_standard_titles_corrected\", 'The_Catcher_in_the_Rye.json'))\n",
    "labels, corpus = annotations_to_pylighter(annotations)\n",
    "annotation = Annotation(corpus, labels_names=[\"PERSON\"], labels=labels, save_path=\"notebooks/annotations/The_Catcher_in_the_Rye.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "referenced-jones",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_path = \"notebooks/annotations/The_Catcher_in_the_Rye.csv\"\n",
    "json_path = \"test_person_gold_standard/The_Catcher_in_the_Rye.json\"\n",
    "csv_to_json(csv_path, json_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "outstanding-input",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
