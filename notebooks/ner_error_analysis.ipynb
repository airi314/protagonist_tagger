{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e951ec95",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"..\")\n",
    "\n",
    "import json\n",
    "import pandas as pd\n",
    "import ast\n",
    "import tabulate\n",
    "\n",
    "from tool.ner_metrics import *\n",
    "from tool.gender_checker import get_personal_titles\n",
    "from tool.annotations_utils import read_annotations, has_intersection, fix_personal_titles, personal_titles_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8edfcb6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "gold_standard_path = \"data/testing_sets/test_person_gold_standard_corrected\"\n",
    "titles = os.listdir(gold_standard_path)\n",
    "annotations_person = read_annotations(os.path.join(gold_standard_path, titles[0]))\n",
    "\n",
    "prediction_path = \"experiments/ner_fixed/flair__ner-large/\"\n",
    "predictions_person = read_annotations(os.path.join(prediction_path, titles[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28074cfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "entities_gold, sentences = data_from_json(os.path.join(gold_standard_path, titles[0]))\n",
    "entities_matcher, _ = data_from_json(os.path.join(prediction_path, titles[0]))\n",
    "entities_gold = [[list(x) for x in set(tuple(x) for x in sent_gold_entities)] for sent_gold_entities in\n",
    "                 entities_gold]\n",
    "gold, matcher, errors = organize_entities(entities_gold, entities_matcher, sentences, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3bf5021",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_errors_stats(gold_path, pred_path):\n",
    "    results = []\n",
    "    for title in os.listdir(os.path.join(gold_path)):\n",
    "        gold_annotations = read_annotations(os.path.join(gold_path, title))\n",
    "        pred_annotations = read_annotations(os.path.join(pred_path, title))\n",
    "        \n",
    "        gold_count = 0\n",
    "        pred_count = 0\n",
    "        exact_count = 0\n",
    "        wrong_hero_count = 0\n",
    "        intersections_count = 0\n",
    "        incorrect_count = 0\n",
    "        missing_count = 0\n",
    "\n",
    "        for gold, pred in zip(gold_annotations, pred_annotations):\n",
    "            gold_entities = gold['entities']\n",
    "            pred_entities = pred['entities']\n",
    "\n",
    "            gold_count += len(gold_entities)\n",
    "            pred_count += len(pred_entities)\n",
    "\n",
    "            matched_count = 0\n",
    "            for entity1 in gold_entities:\n",
    "                exact = False\n",
    "                intersection = False\n",
    "\n",
    "                for entity2 in pred_entities:\n",
    "                    if entity1 == entity2:\n",
    "                        exact_count += 1\n",
    "                        exact = True\n",
    "                        matched_count += 1\n",
    "                        break\n",
    "\n",
    "                if not exact:\n",
    "                    for entity2 in pred_entities:\n",
    "                        if has_intersection(entity1, entity2):\n",
    "                            intersections_count += 1\n",
    "                            intersection = True\n",
    "                            matched_count += 1\n",
    "                            break\n",
    "\n",
    "                if not exact and not intersection:\n",
    "                    missing_count += 1\n",
    "\n",
    "            incorrect_count += len(pred_entities) - matched_count\n",
    "\n",
    "            assert gold_count + incorrect_count == pred_count + missing_count\n",
    "            assert gold_count - exact_count - intersections_count - wrong_hero_count \\\n",
    "                   == missing_count\n",
    "        \n",
    "        title_results = {'Title': title.split('.')[0].replace('_', ' '), \n",
    "                         '# goldstandard': gold_count, \n",
    "                         '# prediction': pred_count, \n",
    "                         '# incorrect annotations': incorrect_count,\n",
    "                         '# annotations with wrong boundaries': intersections_count, \n",
    "                         '# missing annotations':  missing_count}\n",
    "        results.append(title_results)\n",
    "    return pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2850e5c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = get_errors_stats(gold_standard_path, prediction_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07376ca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tabulate.tabulate(df, headers = df.columns, tablefmt='latex_booktabs'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
