{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"..\")\n",
    "\n",
    "import json\n",
    "import numpy as np\n",
    "import re\n",
    "from copy import deepcopy\n",
    "import pandas as pd\n",
    "import tabulate\n",
    "\n",
    "from tool.file_and_directory_management import read_file_to_list, read_sentences_from_file\n",
    "from tool.gender_checker import get_personal_titles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_test_data(sentences_per_novel = 5, save_path = None):\n",
    "    personal_titles = get_personal_titles()\n",
    "    words_re = re.compile('\\\\b('+\"|\".join(personal_titles)+\")(?= )\")\n",
    "    test_sentences = []\n",
    "    for book in read_file_to_list('data/novels_titles/combined_set.txt'):\n",
    "        sentences = read_sentences_from_file(os.path.join('data/testing_sets', 'test', book))\n",
    "        ps_sentences = []\n",
    "        for sent in sentences:\n",
    "            found_titles = words_re.findall(sent)\n",
    "            if len(found_titles) == 1:\n",
    "                ps_sentences.append(sent)\n",
    "        ps_sentences = np.random.choice(ps_sentences, min(sentences_per_novel, len(ps_sentences)), False)\n",
    "        for sent in ps_sentences:\n",
    "            found_title = words_re.findall(sent)[0]\n",
    "            for ps in personal_titles:\n",
    "                new_sent = re.sub(found_title, ps + ' ', sent)\n",
    "                test_sentences.append(new_sent)\n",
    "                    \n",
    "    if save_path is not None:\n",
    "        if not os.path.exists(os.path.dirname(save_path)):\n",
    "            os.makedirs(os.path.dirname(save_path))\n",
    "            \n",
    "        with open(save_path, 'w') as f:\n",
    "            f.write('\\n'.join(test_sentences))\n",
    "            \n",
    "    return test_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sentences = generate_test_data(5, 'data/experiments/personal_titles')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gettig statistics (when predictions are computed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_stats(experiments_dir = 'experiments/personal_titles/ner'):\n",
    "    \n",
    "    stats_dict = {}\n",
    "    personal_titles = get_personal_titles()\n",
    "    words_re = re.compile('\\\\b('+\"|\".join(personal_titles)+\")(?= )\")\n",
    "\n",
    "    for model in os.listdir(experiments_dir):\n",
    "        model_stats_dict = {}\n",
    "        for ps in personal_titles:\n",
    "            model_stats_dict[ps] = [0,0]\n",
    "\n",
    "        with open(os.path.join(experiments_dir, model, 'personal_titles.json')) as f:\n",
    "            data = json.loads(f.read())\n",
    "\n",
    "        for sent_id, sent in enumerate(data):\n",
    "            for e in sent['entities']:\n",
    "                entity_text = sent['content'][e[0]:e[1]]\n",
    "                found_titles = words_re.findall(entity_text)\n",
    "                if any(found_titles):\n",
    "                    model_stats_dict[found_titles[0]][0] += 1\n",
    "                else:\n",
    "                    found_previous_titles = words_re.findall(sent['content'][e[0]-10:e[0]])\n",
    "                    if any(found_previous_titles):\n",
    "                        model_stats_dict[found_previous_titles[0]][1] += 1\n",
    "        stats_dict[model] = model_stats_dict\n",
    "    return stats_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_stats = get_model_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in model_stats.keys():\n",
    "    for subkey in model_stats[key].keys():\n",
    "        if any(model_stats[key][subkey]):\n",
    "            model_stats[key][subkey] = round(100*model_stats[key][subkey][0]/(model_stats[key][subkey][0]+model_stats[key][subkey][1]))\n",
    "        else:\n",
    "            model_stats[key][subkey] = '-'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(model_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_table = pd.DataFrame(model_stats)[['nltk', 'spacy__en_core_web_lg', 'flair__ner-large', 'stanza']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tabulate.tabulate(results_table, headers = ['Personal Title', 'nltk', 'spacy__en_core_web_lg', 'flair__ner-large', 'stanza'], tablefmt='latex_booktabs'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
